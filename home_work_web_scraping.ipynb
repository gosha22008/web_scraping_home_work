{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Обязательная часть\n",
        "Вам необходимо написать функцию, которая будет основана на поиске по сайту habr.com. Функция в качестве параметра должна принимать список запросов для поиска (например, ['python', 'анализ данных']) и на основе материалов, попавших в результаты поиска по каждому запросу, возвращать датафрейм вида:\n",
        "\n",
        "<дата> - <заголовок> - <ссылка на материал>\n",
        "В рамках задания предполагается работа только с одной (первой) страницей результатов поисковой выдачи для каждого запроса. Материалы в датафрейме не должны дублироваться, если они попадали в результаты поиска для нескольких запросов из списка.\n",
        "\n",
        "Дополнительная часть (необязательная)\n",
        "Функция из обязательной части задания должна быть расширена следующим образом:\n",
        "\n",
        "кроме списка ключевых слов для поиска необходимо объявить параметр с количеством страниц поисковой выдачи. Т.е. при передаче в функцию аргумента 4 необходимо получить материалы с первых 4 страниц результатов;\n",
        "в датафрейме должны быть столбцы с полным текстом найденных материалов и количеством лайков:\n",
        "<дата> - <заголовок> - <ссылка на материал> - <текст материала> - <количество лайков>"
      ],
      "metadata": {
        "id": "hFozOpbBWwKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "list_of_search_queries = ['python', 'анализ данных']\n",
        "number_of_pages = 2\n",
        "\n",
        "\n",
        "def pars(list_of_search_queries=['анализ данных', 'python'], number_of_pages=1):\n",
        "  url = 'https://habr.com/ru/search/'\n",
        "  page = 'page'\n",
        "  habr = 'https://habr.com'\n",
        "\n",
        "  res_df = pd.DataFrame(columns=['date', 'title', 'href', 'likes', 'text'])\n",
        "\n",
        "\n",
        "  for name in list_of_search_queries:\n",
        "    \n",
        "\n",
        "    for number in range(1, number_of_pages + 1):\n",
        "      \n",
        "      link = url + page + str(number) + '/'\n",
        "      res = requests.get(link, {f'q': {name},\n",
        "                                'target_type': 'posts',\n",
        "                                'order': 'date'})\n",
        "      soup = BeautifulSoup(res.text)\n",
        "      articles = soup.find_all('article', class_=\"tm-articles-list__item\")\n",
        "\n",
        "      for article in articles:\n",
        "        \n",
        "        title = article.find('a', class_=\"tm-article-snippet__title-link\")\n",
        "        href = habr + title.attrs.get('href')\n",
        "        dat = article.find('time').attrs.get('title')\n",
        "        likes = article.find('div', class_=\"tm-votes-meter tm-data-icons__item\").text.split(' ')\n",
        "        likes = likes[-1]\n",
        "\n",
        "        req = requests.get(href)\n",
        "        soup = BeautifulSoup(req.text)\n",
        "        text = soup.find('div', id=\"post-content-body\").text\n",
        "\n",
        "        row = {'date': dat, 'title': title.text, 'href': href, 'likes': likes, 'text': text}\n",
        "\n",
        "        res_df = pd.concat([res_df, pd.DataFrame([row])]) \n",
        "\n",
        "  res_df = res_df.drop_duplicates()\n",
        "  return res_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "  \n",
        "pars(list_of_search_queries, number_of_pages)\n"
      ],
      "metadata": {
        "id": "dr4LFzKHWvqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}